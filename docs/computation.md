# WIP

Neural networks' fundamental output will be source code "written in" a general purpose programming language. The argument is from first principles and emerges from an idea called _computational irreducibility_.

A computationally irreducible system is a system whose future state at time X + T can only be predicted by computing each state X, X + 1, ..., X + T - 1 according to a set of known transition rules. Such a system is fundamentally intractable, there's no way to compute final states without computing each intermediate state. Such a system affords no intelligent behavior, no way to make predictions. And the thing is, _every single system is computationally irreducible_. 

But intelligence exists. How? By encoding and simulation. An encoding of a system is a correspondence between its state space and a new state space. An encoding can be useful in two ways: it can be small and/or it can be compatible with a useful simulator. A simulator is a machine which computes state transitions. A simulator can be useful in two ways: it can be fast or it can be compatible with a useful encoding.

_To make a prediction about a system, we devise an encoding, encode the current state, and run the encoded state through a simulator._ We use the simulation's output to make inferences about the original system. Encodings and simulators are almost always nested, and the full process of intelligent behavior is in reality a deep and wide tree of encoding/decoding and simulation/inference. This formulation describes everything from speaking and writing to political organization and computer programming.

A pairing of an encoding and a simulator is called an abstraction. So you could say intelligence exists "by abstraction". Useful abstractions are necessarily approximate (semantically lossy) or expensive (energetically lossy). There's exactly one abstraction that is neither lossy nor expensive. We call it "the universe"!

When we create an abstraction we make decisions about the details of its lossiness, these details have a profound impact on how we use the abstraction. _An abstraction's lossiness is characterized by its speed, precision, accuracy, generality, and cost._ A fast abstraction can get "way ahead" of the original system, we can use it to predict states deep in the future. An example is the supercomA precise abstraction is easy to simulate, we can step between its states with a very high level of confidence. An accurate abstraction is easy to infer from, there's a clear correspondence between its states and the original system's. A general abstraction can be used to make predictions about a wide range of the original system's state space. A costly abstraction has an expensive simulator, computing each new state is difficult. An abstraction can not be all at once optimally fast, precise, accurate, general, and cheap because an optimally fast, precise, accurate, general, and cheap abstraction would not be an abstraction, it would be an exact replica.

Sometimes an abstraction's encoding and simulator are not so clearly distinct. We can call them "homoiconic" abstractions. Sometimes an abstraction's encoding seems to be nested within itself. We can call them "recursive" abstractions. Sometimes an abstraction's simulator seems to produce itself. We can call them "autopoietic" abstractions. Sometimes an abstraction tends to produce other abstractions. We can call them "branching" abstractions. _A good definition of life is "a homoiconic, recursive, auotpoietic abstraction". A good definition of intelligence is "life that's also branching"_.

So why do I think neural networks will output source code? I start by considering known intelligence (humanity) as this fundamentally iterative, recursive, homoiconic, autopoietic, and branching tree of encoding/simulation processes evolving through time. I think about the fundamental abstractions we embody and interact with and how they fit together, what they optimize and what they lose. Then I think about how the optimization choices might be different for an in silico intelligence.

First, we have our nervous systems. Our nervous systems are imprecise and inaccurate. But they're also very cheap and extremely general. It's turned out that they're also very slow (as shown by the relative speed of digital neural networks). They're imprecise in that streams of thought tend to be highly erratic and unstructured. It's only with great effort that we can maintain a sequence of thoughts which at all resembles a logically-consistent progression. They're inaccurate in that it's not at all guaranteed that any given mental state corresponds to a physically-true reality. They're extremely cheap in that the resources needed to keep one of us running is approximately equivalent to the resources one of us has access to by default. They're extremely general almost by definition: we (for the most part) don't usually care about things beyond our ken. Beyond that, we've developed mathematics and general purpose computing devices, whose analytical functionality seems to extend (in theory) to all physical phenomena.

It seems like nervous systems' generality is somehow related to their homoiconicity. Nervous system's can represent not only an encoding but also a simulator. Natural language is an example. Natural language is a lossy encoding of physical reality that improves on our nervous system's precision and accuracy, but it's also slightly slower and less general. Because natural language is more precise than our raw impulses, it allows us to _think deeply_. When we use our nervous systems to simulate reality using natural language, we have more confidence in the logical coherence of each state of the simulation. Books make a lot more sense than an equivalent stream of consciousness.

Now let's consider "number". Number is an encoding which optimizes precision and accuracy at the cost of generality. We pair number with mathematics, a simulator which runs sometimes in our heads and sometimes inside computing machines. With number and mathematics we can build towering simulations of essentially anything we can measure. The drawback to mathematics is that number is an extremely narrow encoding which requires extremely complex state transition rules to be useful.

The next fundamental abstraction we'll consider is the "bit". The bit represents a fundamental aspect of physical reality: a thing either is or isn't but is never neither and never both (quantum mechanics doesn't disagree, it just changes what we mean by "is" or "isn't").

...