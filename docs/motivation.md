# Gadfly

Within 3 years every desk job with a median salary of less than $50K will be implementable by a computer program. Within 10 years, $100K. The source code for these programs will be written in your favorite general purpose programming language, but not by humans. Instead, they'll be compiled from natural language problem statements and other high-level specifications. I call the compilers _system 2 compilers_.

A system 2 compiler is a computer program which provides to neural networks the primitives they need in order to deliberate, plan, adapt, and generate effects. "System 2" thinking is a term of art which refers to slow, intentional, conscious, higher-order thinking as opposed to "system 1" thinking which refers to fast, automatic, reflexive thinking. A light conversation with your friend requires system 1 thinking, opening a restaurant requires system 2 thinking. A system 2 compiler outfits neural networks with a system 2. It may end up the case that system 2 compilers provide the grounding and/or embodiment many people believe is necessary to achieve AGI.

System 2 compilers will produce many orders of magnitude more software than exists in the world today. On the one hand we now have programs that can read and write and see, blowing open the theoretical scope of software functionality. On the other hand we can generate source code, say, 1,000,000 times quicker than we could last year. System 2 compilers will commoditize the 100,000 lines-of-code codebase and, eventually, the price of software will converge to the price of compute. The platforms built around system 2 compilers will be worth trillions of dollars.

But what about the bitter lesson? Doesn't it tell us that system 2 compilers will be a fleeting phenomenon at most? The bitter lesson tells us that if we try to use what we think we know about how we think in order to improve our AI systems, our work will be obsolesced by work which scales with compute. But a system 2 compiler works in exactly the opposite direction: it equips neural networks with _formal reasoning_ engines (i.e. general purpose programming languages). 

Couldn't we just shove everything inside a neural network? Probably not. What will probably happen is the scope of neural networks will continue to increase and system 2 compilers will evolve along with them. The argument is from first principles and centers on this idea called _computational irreducibility_. You can find it fleshed out [here](/).

System 2 compilers will share a common architecture: a coordination engine, an execution engine, and a simulation engine, all driven by neural networks. The coordination engine will decompose and recompose problems. The execution engine will generate, run, and test source code. The simulation engine will manage convergence (more on this [here](/). Neural networks will parse and generate natural language and discretize continuous search trees.

Programs generated by system 2 compilers will be intelligent applications, or applications that can reason. In the beginning most of the intelligent applications will be just like regular applications but with their functional scopes extended to include features that require reading and writing and/or decision making against very fuzzy parameters. Many of the intelligent applications will be copilots, programs that watch what you're doing, anticipate your needs, and proactively fetch and produce useful context. Eventually system 2 compilers will be used to build _AI agents_, intelligent applications that also write themselves.

The question is not whether system 2 compilers will be built, it's a question of how soon and by whom. It's not 100% certain that the current generation of foundation models can power a system 2 compiler, but it's also not at all certain that they can't. It is, however, certain that some upcoming generation of language models will work. The platforms built around system 2 compilers will be worth trillions of dollars and we need to get started working on them now.

- 1980 people who wanted a computation done had to go to a programmer
- Mathematica/Wolfram Alpha
- linguistic interfaces broaden access to computation
- slabs of boilerplate become a single function
- boilerplate programming goes the way of assembly
- natural language -> computational language
- vastly more people will care about computation. art teachers will use computation more
- "why does the picture look kind of off"
- math (learn it before you can use it) compu lang (use it then learn it)
- will people never leanr compu lang
- what should people learn (cx computational thinking)
- lots of fields were became computational (software eng uni starting in the 90s)
- you need know where to drive the car (encoding/decoding ideas)
- cx is a way of formalizing the world, kind of like what logic was back in the day. the growth of systematic data (systematic description of certain things (encodings)) a formal way so that you can build up
- we implement it with computers but what it really is a formalism. it is constrained but it can expand to cover vast spaces
- will language evolve into computational language?
- language is optimized for shallow context, it doesn't do a bunch of deep nesting because our brains don't do that well. interestingly, chatgpt is good and bad at the same things (it doesn't do paren matchign well)
- formalism is sometimes math, sometimes programming, etc
- learn about encodings
- the computationalization of the world should be part of standard education
- testing is part of it, it's not just encodings
- analysis, breakdown, reduction, expansion, etc
- it's kind of a set of design patterns that end up showing up everywhere as you write computer programs. it's the kind of computational structure of the world. not the theoretical, the actual in fact practical logical structure of the world
- writing started to be a specialty, then it became a thing for everyone, math started to be a specialty, then it became a thing, programming started to be a specialty, then it became a thing. we had quills and parchment and bullshit, but then it became accessible. we had compilers and programs and whatnot but now we have language models. we will expect everybody to know how to do cx because it will be available to everybody
- the visual arts will be able to do something like "tell me what % of the screen is white".